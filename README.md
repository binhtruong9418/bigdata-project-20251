# Zillow Real Estate Data Pipeline - NhÃ³m 14

[BÃ¡o cÃ¡o BTL BigData - NhÃ³m 14 - Lá»›p 154050](https://docs.google.com/document/d/1Svi3nbpFZvkNQm9AJzbJgJ29YFHlan6-cYaCBs4nb8U/edit?tab=t.0)

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n real-time sá»­ dá»¥ng cÃ´ng nghá»‡ Big Data. Pipeline bao gá»“m:

1. **Data Ingestion**: Táº¡o dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t California vÃ  gá»­i vÃ o Kafka
2. **Stream Processing**: Xá»­ lÃ½ real-time vá»›i Spark Structured Streaming
3. **Batch Processing**: LÆ°u trá»¯ batch data vÃ o HDFS vÃ  Cassandra
4. **Machine Learning**: Train model vÃ  dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i Random Forest
5. **Analytics**: Aggregation theo thÃ nh phá»‘, home type, vÃ  thá»‘ng kÃª chi tiáº¿t

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
Producer (Python)
    â†“
Kafka Cluster (3 brokers)
    â†“
    â”œâ”€â”€ Spark Structured Streaming â†’ Console Output (Real-time Analytics)
    â”‚
    â””â”€â”€ Batch Consumer â†’ HDFS
            â†“
        Batch Processing (PySpark)
            â†“
        Cassandra â†’ ML Prediction (predict_prices.py)
```

### CÃ¡c thÃ nh pháº§n:

- **Kafka Producer**: Sinh dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t (price, bedrooms, city, ...)
- **Kafka Cluster**: 3 brokers (ports 9092, 9093, 9094) + ZooKeeper
- **Spark Streaming**: Xá»­ lÃ½ real-time, tÃ­nh toÃ¡n metrics theo time window
- **Batch Consumer**: Äá»c tá»« Kafka, lÆ°u batch vÃ o HDFS
- **HDFS**: LÆ°u trá»¯ phÃ¢n tÃ¡n dá»¯ liá»‡u batch
- **Batch Processing**: Load data tá»« HDFS vÃ o Cassandra (tá»± Ä‘á»™ng setup database)
- **Cassandra**: NoSQL database cho ML pipeline
- **ML Prediction**: Train model vÃ  dá»± Ä‘oÃ¡n giÃ¡ cho táº¥t cáº£ properties (Random Forest)

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m cáº§n thiáº¿t:
- **Python 3.11** (KHUYáº¾N NGHá»Š - Full compatibility)
- **Java** >= 8 (JDK)
- **Docker** vÃ  **Docker Compose**
- **Git** (Ä‘á»ƒ clone project)

**Táº¡i sao Python 3.11?**
- âœ… Há»— trá»£ Ä‘áº§y Ä‘á»§ táº¥t cáº£ thÆ° viá»‡n (Kafka, PySpark, Cassandra)
- âœ… cassandra-driver hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… PySpark 3.5.0 stable vá»›i Cassandra connector
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i toÃ n bá»™ Big Data stack

**LÆ°u Ã½**:
- Python 3.13 chÆ°a Ä‘Æ°á»£c cassandra-driver há»— trá»£
- PySpark 4.0+ chÆ°a tÆ°Æ¡ng thÃ­ch vá»›i Cassandra Spark Connector
- Sá»­ dá»¥ng Python 3.11 + PySpark 3.5.0 Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» tÆ°Æ¡ng thÃ­ch

### Há»‡ Ä‘iá»u hÃ nh:
- Windows 10/11, Linux, hoáº·c MacOS

---

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone project

```bash
git clone <repository-url>
cd bigdata-project-20251
```

### BÆ°á»›c 2: Táº¡o mÃ´i trÆ°á»ng áº£o vÃ  cÃ i Ä‘áº·t dependencies

#### Windows:
```bash
python -m venv .venv
.venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

#### Linux/Mac:
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

**LÆ°u Ã½ quan trá»ng**: Náº¿u gáº·p lá»—i import khi cháº¡y PySpark, xÃ³a `.venv` vÃ  táº¡o láº¡i tá»« Ä‘áº§u:
```bash
# Windows
rmdir /s /q .venv
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Linux/Mac
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### BÆ°á»›c 3: Setup Hadoop cho Windows (chá»‰ Windows)

ÄÃ£ Ä‘Æ°á»£c tá»± Ä‘á»™ng setup trong code. Hadoop binaries sáº½ Ä‘Æ°á»£c táº£i tá»± Ä‘á»™ng vÃ o folder `hadoop/bin/`.

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Docker services

```bash
docker-compose up -d
```

Chá» khoáº£ng 30-60 giÃ¢y Ä‘á»ƒ cÃ¡c services khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

### BÆ°á»›c 5: Kiá»ƒm tra cÃ¡c services

```bash
docker-compose ps
```

Äáº£m báº£o táº¥t cáº£ containers Ä‘ang cháº¡y (status: Up).

---

## ğŸ¯ Cháº¡y dá»± Ã¡n

### Workflow hoÃ n chá»‰nh:

```
1. Start Docker services
2. Run Producer (táº¡o data)
3. Run Batch Consumer (lÆ°u vÃ o HDFS)
4. Run Streaming Consumer (real-time analytics)
5. Wait 10-15 minutes (collect data)
6. Run Batch Processing (HDFS â†’ Cassandra, auto setup database)
7. Run ML Prediction (train model vÃ  predict ALL data tá»« Cassandra)
```

### CÃ¡ch 1: Cháº¡y tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

#### Windows:
```bash
.\start.bat
```

#### Linux/Mac:
```bash
chmod +x start.sh
./start.sh
```

Script sáº½ tá»± Ä‘á»™ng cháº¡y:
- Kafka Producer
- Batch Consumer (HDFS)
- Spark Streaming Consumer

### CÃ¡ch 2: Cháº¡y tá»«ng thÃ nh pháº§n riÃªng láº»

#### Terminal 1 - Kafka Producer:
```bash
python kafka/producer.py
```

#### Terminal 2 - Batch Consumer (HDFS):
```bash
python kafka/consumer_batch.py
```

#### Terminal 3 - Streaming Consumer (Spark):
```bash
python kafka/consumer_structured_stream.py
```

---

## ğŸ¤– Machine Learning Workflow

### BÆ°á»›c 1: Collect data (Ä‘á»£i 10-15 phÃºt)
```bash
# Producer, Batch Consumer, vÃ  Spark Streaming Ä‘ang cháº¡y
# Äá»£i Ä‘á»ƒ data Ä‘Æ°á»£c collect vÃ o HDFS
# Kiá»ƒm tra: http://localhost:9870 â†’ Utilities â†’ Browse the file system â†’ /data/kafka_messages
```

### BÆ°á»›c 2: Load data tá»« HDFS vÃ o Cassandra
```bash
python spark/batch_processing.py
```

**TÃ­nh nÄƒng:**
- âœ… Tá»± Ä‘á»™ng táº¡o Cassandra keyspace `finaldata1`
- âœ… Tá»± Ä‘á»™ng táº¡o table `data2` vá»›i schema phÃ¹ há»£p
- âœ… KhÃ´ng cáº§n cháº¡y setup riÃªng
- âœ… Hiá»ƒn thá»‹ progress bar khi xá»­ lÃ½ nhiá»u files
- âœ… Error handling tá»‘t, retry logic
- âœ… Summary report chi tiáº¿t (success/failed counts)

**Output máº«u:**
```
============================================================
Zillow Batch Processing - HDFS to Cassandra
============================================================

[Setup] Configuring Cassandra database...
[OK] Cassandra keyspace 'finaldata1' and table 'data2' ready

[1/4] Connecting to HDFS...
[OK] Connected to HDFS

[2/4] Searching for data files...
[OK] Found 2984 files to process

[3/4] Initializing Spark session...
[OK] Spark session created

[4/4] Processing and loading data to Cassandra...
[Progress] Processed 100/2984 files...
[Progress] Processed 200/2984 files...

============================================================
BATCH PROCESSING COMPLETED
============================================================

Total files found: 2984
Successfully processed: 2980
Failed: 4

Data written to: Cassandra keyspace 'finaldata1', table 'data2'
```

### BÆ°á»›c 3: Train Model & Predict ALL Data
```bash
python spark/predict_prices.py
```

**TÃ­nh nÄƒng má»›i:**
- âœ… Reads ALL data from Cassandra automatically
- âœ… Trains Random Forest model (20 trees, depth 10)
- âœ… Predicts prices for ALL properties
- âœ… Shows sample predictions (first 20)
- âœ… Statistics by city and home type
- âœ… **Spark Web UI available at http://localhost:4040**
- âœ… Keeps session alive - press ENTER when done exploring UI
- âœ… No temp file cleanup errors on Windows

**Output máº«u:**
```
============================================================
Zillow Price Prediction - Predict All Data
============================================================

[1/6] Initializing Spark session...
[OK] Spark session created
[INFO] Spark Web UI available at: http://localhost:4040

[2/6] Reading ALL data from Cassandra...
[OK] Loaded 650 properties

[3/6] Preprocessing data...
[OK] 650 valid properties

[4/6] Building ML Pipeline...

[5/6] Training model (this takes 2-3 minutes)...
[OK] Training completed!

[6/6] Making predictions on all properties...
[OK] Predictions completed!

============================================================
SAMPLE PREDICTIONS (First 20)
============================================================
+---------+-------------+-------------+------------+--------------------+
|zpid     |city         |hometype     |actual_price|predicted_price     |
+---------+-------------+-------------+------------+--------------------+
|255566386|Burbank      |MANUFACTURED |1787176     |3199940.03          |
|376355892|Beverly Hills|MULTI_FAMILY |8260909     |11710404.13         |
+---------+-------------+-------------+------------+--------------------+

============================================================
OVERALL STATISTICS
============================================================

Total properties: 650
Avg actual price: $7,980,586.12
Avg predicted price: $8,251,029.00
Min predicted: $903,183.28
Max predicted: $67,311,818.57

============================================================
PREDICTIONS BY CITY
============================================================
+--------------+-----+--------------------+
|city          |count|avg_predicted       |
+--------------+-----+--------------------+
|Sherman Oaks  |72   |11889660.33         |
|Beverly Hills |51   |6185828.02          |
+--------------+-----+--------------------+

Spark Web UI: http://localhost:4040

Press ENTER when done exploring the UI...
```

**Model features:**
- Algorithm: Random Forest Regressor
- Trees: 20
- Max Depth: 10
- Features: city, hometype, bedrooms, bathrooms, livingarea, lotareavalue, etc.
- Target: price prediction

---

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Producer Output:
```
Sent data: {'timestamp': 1763174687104, 'zpid': 336424216, 'city': 'Los Angeles', 'price': 7578356, ...}
Sent data: {'timestamp': 1763174687952, 'zpid': 261109533, 'city': 'Glendale', 'price': 7632629, ...}
```

### Batch Consumer Output:
```
2025-11-15 10:06:05,815 - __main__ - INFO - Batch of 10 messages saved to /data/kafka_messages/2025/11/15/10_06_05_843515_batch.json
```

### Spark Streaming Output:
```
+----------------------------------------------+--------------+-------------+-----------------+
|window                                        |city          |average_price|total_bedrooms   |
+----------------------------------------------+--------------+-------------+-----------------+
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Los Angeles   |6022806.0    |6                |
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Beverly Hills |8109473.0    |11               |
+----------------------------------------------+--------------+-------------+-----------------+
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
bigdata-project-20251/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py                  # Kafka producer
â”‚   â”œâ”€â”€ consumer_batch.py            # Batch consumer â†’ HDFS
â”‚   â””â”€â”€ consumer_structured_stream.py # Spark streaming consumer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ batch_processing.py          # HDFS â†’ Cassandra (optimized, auto setup)
â”‚   â”œâ”€â”€ predict_prices.py            # ML prediction for ALL data (with Spark UI)
â”‚   â””â”€â”€ README.md                    # Spark scripts documentation
â”œâ”€â”€ hadoop/
â”‚   â””â”€â”€ bin/                         # Hadoop binaries (Windows only)
â”œâ”€â”€ check_cassandra_data.py          # Verify Cassandra data
â”œâ”€â”€ docker-compose.yml               # Docker services configuration
â”œâ”€â”€ requirements.txt                 # Python dependencies (PySpark 3.5.0)
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ start.bat                        # Windows startup script
â”œâ”€â”€ start.sh                         # Linux/Mac startup script
â”œâ”€â”€ stop.bat                         # Windows stop script
â”œâ”€â”€ stop.sh                          # Linux/Mac stop script
â”œâ”€â”€ CLEANUP.md                       # Complete cleanup guide
â””â”€â”€ README.md                        # This file
```

---

## ğŸŒ Web UIs

Sau khi khá»Ÿi Ä‘á»™ng Docker services, cÃ³ thá»ƒ truy cáº­p:

- **Spark Master**: http://localhost:8080
- **Spark Worker**: http://localhost:8081
- **HDFS NameNode**: http://localhost:9870
  - Browse files: Utilities â†’ Browse the file system â†’ /data/kafka_messages
- **Spark Application UI**: http://localhost:4040 (khi cháº¡y predict_prices.py)

---

## ğŸ“¦ Dependencies chÃ­nh

```
# Core Big Data Processing
pyspark==3.5.0                      # TÆ°Æ¡ng thÃ­ch vá»›i Cassandra connector
py4j==0.10.9.7

# Kafka
kafka-python-ng                      # Kafka client (Python 3.11+ compatible)

# Data Processing
pandas
numpy

# Storage Connectors
hdfs                                 # HDFS client
cassandra-driver                     # Cassandra client (Python 3.11 compatible)
```

**Version Compatibility Matrix**:
| Component | Version | Reason |
|-----------|---------|--------|
| PySpark | 3.5.0 | Compatible with Cassandra connector 3.5.0 |
| Cassandra Connector | 3.5.0 (Scala 2.12) | Matches Spark 3.5.0 Scala version |
| Python | 3.11 | Full cassandra-driver support |

---

## ğŸ› Troubleshooting

### 1. Import Error: `cannot import name 'is_remote_only'`
**NguyÃªn nhÃ¢n**: Virtual environment bá»‹ corrupt vá»›i mixed PySpark versions

**Giáº£i phÃ¡p**:
```bash
# Windows
deactivate
rmdir /s /q .venv
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Linux/Mac
deactivate
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Cassandra Connection Error
**NguyÃªn nhÃ¢n**: Cassandra chÆ°a sáºµn sÃ ng

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Cassandra status
docker-compose ps

# Restart Cassandra
docker-compose restart cassandra

# Äá»£i 30-60 giÃ¢y rá»“i thá»­ láº¡i
```

### 3. Spark Cassandra Connector Error: `NoClassDefFoundError: scala/$less$colon$less`
**NguyÃªn nhÃ¢n**: Sai Scala version trong connector

**Giáº£i phÃ¡p**: ÄÃ£ fix trong code, sá»­ dá»¥ng `_2.12` thay vÃ¬ `_2.13`

### 4. Docker containers khÃ´ng start
```bash
docker-compose down
docker-compose up -d
```

### 5. Port Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng
```bash
# Windows
netstat -ano | findstr :9092
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :9092
kill -9 <PID>
```

### 6. HDFS khÃ´ng accessible
```bash
# Kiá»ƒm tra HDFS NameNode
docker logs namenode

# Restart HDFS
docker-compose restart namenode datanode
```

### 7. Spark temp file cleanup errors (Windows)
**NguyÃªn nhÃ¢n**: Windows file locking

**Giáº£i phÃ¡p**: ÄÃ£ fix - script sá»­ dá»¥ng local `spark-temp/` directory vÃ  graceful shutdown. Errors (náº¿u cÃ³) lÃ  cosmetic vÃ  khÃ´ng áº£nh hÆ°á»Ÿng káº¿t quáº£.

---

## ğŸ”§ Optimizations

### Batch Processing Script (`batch_processing.py`):
1. **Integrated Cassandra Setup**: Tá»± Ä‘á»™ng táº¡o keyspace vÃ  table, khÃ´ng cáº§n script riÃªng
2. **Modular Design**: Functions cho tá»«ng task (HDFS, Spark, Transform)
3. **Better Error Handling**: Try-catch cho tá»«ng file, khÃ´ng dá»«ng náº¿u 1 file lá»—i
4. **Progress Tracking**: Hiá»ƒn thá»‹ progress má»—i 100 files
5. **Summary Report**: Tá»•ng káº¿t success/failed counts

### ML Prediction Script (`predict_prices.py`):
1. **No Model Save/Load**: Train vÃ  predict trong cÃ¹ng session - trÃ¡nh lá»—i Windows
2. **Spark UI Integration**: Web UI available at http://localhost:4040
3. **Keep Session Alive**: Press ENTER to keep UI accessible
4. **Complete Statistics**: Breakdowns by city, home type, overall stats
5. **Windows Optimized**: Local temp dir, clean shutdown, no errors

### Version Compatibility:
- Downgrade tá»« PySpark 4.0.1 â†’ 3.5.0 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i Cassandra connector
- Sá»­ dá»¥ng Scala 2.12 connector thay vÃ¬ 2.13
- Python 3.11 cho full cassandra-driver support

---

## ğŸ—‘ï¸ Cleanup Guide

Xem file `CLEANUP.md` Ä‘á»ƒ biáº¿t cÃ¡ch xÃ³a hoÃ n toÃ n project vÃ  táº¥t cáº£ data.

**Quick cleanup:**
```bash
# Stop all services
docker-compose down -v

# Delete external storage (OUTSIDE project - IMPORTANT!)
cd D:\Workspace\
rmdir /s /q storage

# Delete project
rmdir /s /q bigdata-project-20251

# Clean Spark temp
cd C:\Users\binht\AppData\Local\Temp\
for /d %i in (spark-*) do rmdir /s /q "%i"
```

**Total space freed**: ~11-22 GB

---

## ğŸ‘¥ NhÃ³m thá»±c hiá»‡n

**NhÃ³m 14 - Lá»›p 154050**

---

## ğŸ“ License

Dá»± Ã¡n há»c táº­p - Äáº¡i há»c [TÃªn trÆ°á»ng]

---

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ váº¥n Ä‘á», vui lÃ²ng táº¡o issue trong repository hoáº·c liÃªn há»‡ nhÃ³m.

---

## ğŸ“ Learning Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Cassandra Documentation](https://cassandra.apache.org/doc/)
- [HDFS Architecture](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
- [Spark-Cassandra Connector](https://github.com/apache/cassandra-spark-connector)
- [Random Forest Algorithm](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression)
