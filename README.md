# Zillow Real Estate Data Pipeline - NhÃ³m 14

[BÃ¡o cÃ¡o BTL BigData - NhÃ³m 14 - Lá»›p 154050](https://docs.google.com/document/d/1Svi3nbpFZvkNQm9AJzbJgJ29YFHlan6-cYaCBs4nb8U/edit?tab=t.0)

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n real-time sá»­ dá»¥ng cÃ´ng nghá»‡ Big Data. Pipeline bao gá»“m:

1. **Data Ingestion**: Táº¡o dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t California vÃ  gá»­i vÃ o Kafka
2. **Stream Processing**: Xá»­ lÃ½ real-time vá»›i Spark Structured Streaming
3. **Batch Processing**: LÆ°u trá»¯ batch data vÃ o HDFS
4. **Storage**: PhÃ¢n tÃ¡n dá»¯ liá»‡u trÃªn Cassandra vÃ  Elasticsearch
5. **Analytics**: Aggregation theo thÃ nh phá»‘, thá»i gian

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
Producer (Python)
    â†“
Kafka Cluster (3 brokers)
    â†“
    â”œâ”€â”€ Spark Structured Streaming â†’ Console Output (Real-time Analytics)
    â”‚
    â””â”€â”€ Batch Consumer â†’ HDFS
            â†“
        Batch Processing (PySpark)
            â†“
        Cassandra â†’ ML Training (sparkML.py)
```

### CÃ¡c thÃ nh pháº§n:

- **Kafka Producer**: Sinh dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t (price, bedrooms, city, ...)
- **Kafka Cluster**: 3 brokers (ports 9092, 9093, 9094) + ZooKeeper
- **Spark Streaming**: Xá»­ lÃ½ real-time, tÃ­nh toÃ¡n metrics theo time window
- **Batch Consumer**: Äá»c tá»« Kafka, lÆ°u batch vÃ o HDFS
- **HDFS**: LÆ°u trá»¯ phÃ¢n tÃ¡n dá»¯ liá»‡u batch
- **Batch Processing**: Load data tá»« HDFS vÃ o Cassandra
- **Cassandra**: NoSQL database cho ML pipeline
- **ML Training**: Huáº¥n luyá»‡n model dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (Random Forest)
- **Elasticsearch + Kibana**: Search vÃ  visualization (reserved)

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m cáº§n thiáº¿t:
- **Python 3.11** (KHUYáº¾N NGHá»Š - Full compatibility)
- **Java** >= 8 (JDK)
- **Docker** vÃ  **Docker Compose**
- **Git** (Ä‘á»ƒ clone project)

**Táº¡i sao Python 3.11?**
- âœ… Há»— trá»£ Ä‘áº§y Ä‘á»§ táº¥t cáº£ thÆ° viá»‡n (Kafka, PySpark, Cassandra)
- âœ… cassandra-driver hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… PySpark 4.0.1 stable vÃ  khÃ´ng cÃ³ worker crashes
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i toÃ n bá»™ Big Data stack

**LÆ°u Ã½**: Python 3.13 chÆ°a Ä‘Æ°á»£c cassandra-driver há»— trá»£. Sá»­ dá»¥ng Python 3.11 Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» tÆ°Æ¡ng thÃ­ch.

### Há»‡ Ä‘iá»u hÃ nh:
- Windows 10/11, Linux, hoáº·c MacOS

---

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone project

```bash
git clone <repository-url>
cd bigdata-project-20251
```

### BÆ°á»›c 2: Táº¡o mÃ´i trÆ°á»ng áº£o vÃ  cÃ i Ä‘áº·t dependencies

#### Windows:
```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

#### Linux/Mac:
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### BÆ°á»›c 3: Setup Hadoop cho Windows (chá»‰ Windows)

ÄÃ£ Ä‘Æ°á»£c tá»± Ä‘á»™ng setup trong code. Hadoop binaries sáº½ Ä‘Æ°á»£c táº£i tá»± Ä‘á»™ng vÃ o folder `hadoop/bin/`.

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Docker services

```bash
docker-compose up -d
```

Chá» khoáº£ng 30-60 giÃ¢y Ä‘á»ƒ cÃ¡c services khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

### BÆ°á»›c 5: Kiá»ƒm tra cÃ¡c services

```bash
docker-compose ps
```

Äáº£m báº£o táº¥t cáº£ containers Ä‘ang cháº¡y (status: Up).

---

## ğŸ¯ Cháº¡y dá»± Ã¡n

### CÃ¡ch 1: Cháº¡y tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

#### Windows:
```bash
.\start.bat
```

#### Linux/Mac:
```bash
chmod +x start.sh
./start.sh
```

### CÃ¡ch 2: Cháº¡y tá»«ng thÃ nh pháº§n riÃªng láº»

#### Terminal 1 - Kafka Producer:
```bash
python kafka/producer.py
```

#### Terminal 2 - Batch Consumer (HDFS):
```bash
python kafka/consumer_batch.py
```

#### Terminal 3 - Streaming Consumer (Spark):
```bash
python kafka/consumer_structured_stream.py
```

### Machine Learning:

**LÆ°u Ã½**: sparkML.py Ä‘á»c dá»¯ liá»‡u tá»« Cassandra Ä‘á»ƒ train model dá»± Ä‘oÃ¡n giÃ¡ nhÃ .

ML training workflow bao gá»“m cÃ¡c bÆ°á»›c sau:

#### BÆ°á»›c 1: Cháº¡y pipeline Ä‘á»ƒ collect data (Ä‘ang cháº¡y tá»« start.bat/start.sh)
```bash
# Producer, Batch Consumer, vÃ  Spark Streaming Ä‘ang cháº¡y
# Äá»£i 10-15 phÃºt Ä‘á»ƒ data Ä‘Æ°á»£c collect vÃ o HDFS
```

#### BÆ°á»›c 2: Load data tá»« HDFS vÃ o Cassandra
```bash
python spark/batch_processing.py
```

#### BÆ°á»›c 3: Kiá»ƒm tra Cassandra cÃ³ Ä‘á»§ data chÆ°a
```bash
python check_cassandra_data.py
```

Script nÃ y sáº½:
- Káº¿t ná»‘i tá»›i Cassandra
- Äáº¿m sá»‘ lÆ°á»£ng records trong table `finaldata1.data2`
- Hiá»ƒn thá»‹ sample data
- ÄÆ°a ra khuyáº¿n nghá»‹ cÃ³ nÃªn train ML model hay chÆ°a

**Khuyáº¿n nghá»‹ sá»‘ lÆ°á»£ng data:**
- Tá»‘i thiá»ƒu: 100-200 rows
- Tá»‘i Æ°u: 1000+ rows

#### BÆ°á»›c 4: Train ML model vá»›i dá»¯ liá»‡u thá»±c tá»« Cassandra
```bash
# Train model dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i Random Forest
python spark/sparkML.py
```

### Dá»«ng há»‡ thá»‘ng:

```bash
# Nháº¥n Ctrl+C á»Ÿ má»—i terminal Ä‘á»ƒ dá»«ng cÃ¡c consumer/producer

# Dá»«ng Docker services
docker-compose down
```

---

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Producer Output:
```
Sent data: {'timestamp': 1763174687104, 'zpid': 336424216, 'city': 'Los Angeles', 'price': 7578356, ...}
Sent data: {'timestamp': 1763174687952, 'zpid': 261109533, 'city': 'Glendale', 'price': 7632629, ...}
```

### Batch Consumer Output:
```
2025-11-15 10:06:05,815 - __main__ - INFO - Batch of 10 messages saved to /data/kafka_messages\2025\11\15\10_06_05_843515_batch.json
```

### Spark Streaming Output:
```
+----------------------------------------------+--------------+-------------+-----------------+
|window                                        |city          |average_price|total_bedrooms   |
+----------------------------------------------+--------------+-------------+-----------------+
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Los Angeles   |6022806.0    |6                |
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Beverly Hills |8109473.0    |11               |
+----------------------------------------------+--------------+-------------+-----------------+
```

---

## ğŸ”§ CÃ¡c lá»—i Ä‘Ã£ fix vÃ  giáº£i phÃ¡p

### 1. âŒ Lá»—i kafka-python module
**Lá»—i**: `ModuleNotFoundError: No module named 'kafka.vendor.six.moves'`

**NguyÃªn nhÃ¢n**: Package `kafka-python` khÃ´ng cÃ²n Ä‘Æ°á»£c maintain

**Giáº£i phÃ¡p**: Thay báº±ng `kafka-python-ng`
```bash
pip uninstall kafka-python -y
pip install kafka-python-ng
```

### 2. âŒ Lá»—i Kafka Docker
**Lá»—i**: `KAFKA_PROCESS_ROLES is not set`

**NguyÃªn nhÃ¢n**: Kafka image `latest` máº·c Ä‘á»‹nh dÃ¹ng KRaft mode

**Giáº£i phÃ¡p**: Äá»•i sang version `7.4.0` há»— trá»£ ZooKeeper
```yaml
image: confluentinc/cp-kafka:7.4.0
```

### 3. âŒ Lá»—i PySpark typing
**Lá»—i**: `ModuleNotFoundError: No module named 'typing.io'`

**NguyÃªn nhÃ¢n**: Python 3.13 khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i PySpark 3.3.2

**Giáº£i phÃ¡p**: Upgrade PySpark lÃªn 4.0.1
```bash
pip install --upgrade pyspark
```

### 4. âŒ Lá»—i HADOOP_HOME
**Lá»—i**: `HADOOP_HOME and hadoop.home.dir are unset`

**NguyÃªn nhÃ¢n**: Windows cáº§n Hadoop binaries

**Giáº£i phÃ¡p**: Tá»± Ä‘á»™ng download winutils.exe vÃ  set HADOOP_HOME trong code

### 5. âŒ Lá»—i Kafka API Version
**Lá»—i**: `NoBrokersAvailable` vá»›i consumer_batch.py

**NguyÃªn nhÃ¢n**: Timeout khi detect API version

**Giáº£i phÃ¡p**: Set explicit API version
```python
api_version=(2, 8, 1),
request_timeout_ms=30000
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
bigdata-project-20251/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py                  # Kafka producer
â”‚   â”œâ”€â”€ consumer_batch.py            # Batch consumer â†’ HDFS
â”‚   â””â”€â”€ consumer_structured_stream.py # Spark streaming consumer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ batch_processing.py          # Spark batch processing â†’ Cassandra
â”‚   â”œâ”€â”€ sparkML.py                   # ML model training
â”‚   â””â”€â”€ sparkML_note.txt             # Python compatibility notes
â”œâ”€â”€ hadoop/
â”‚   â””â”€â”€ bin/                         # Hadoop binaries (auto-downloaded)
â”œâ”€â”€ check_cassandra_data.py          # Check Cassandra data before ML
â”œâ”€â”€ docker-compose.yml               # Docker services config
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ start.bat                        # Windows startup script
â”œâ”€â”€ start.sh                         # Linux/Mac startup script
â”œâ”€â”€ stop.bat                         # Windows stop script
â”œâ”€â”€ stop.sh                          # Linux/Mac stop script
â”œâ”€â”€ QUICKSTART.md                    # Quick start guide
â””â”€â”€ README.md                        # File nÃ y
```

---

## ğŸŒ Web UIs

Sau khi khá»Ÿi Ä‘á»™ng Docker services, cÃ³ thá»ƒ truy cáº­p:

- **Spark Master**: http://localhost:8080
- **Spark Worker**: http://localhost:8081
- **HDFS NameNode**: http://localhost:9870
- **Elasticsearch**: http://localhost:9200
- **Kibana**: http://localhost:5601

---

## ğŸ“¦ Dependencies chÃ­nh

```
pyspark==4.0.1
kafka-python-ng
hdfs
cassandra-driver
pandas
numpy
```

---

## ğŸ› Troubleshooting

### Docker containers khÃ´ng start:
```bash
docker-compose down
docker-compose up -d
```

### Port Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng:
Kiá»ƒm tra vÃ  kill process Ä‘ang dÃ¹ng port:
```bash
# Windows
netstat -ano | findstr :9092
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :9092
kill -9 <PID>
```

### Kafka connection timeout:
Äá»£i thÃªm 30-60 giÃ¢y Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

---

## ğŸ‘¥ NhÃ³m thá»±c hiá»‡n

**NhÃ³m 14 - Lá»›p 154050**

---

## ğŸ“ License

Dá»± Ã¡n há»c táº­p - Äáº¡i há»c [TÃªn trÆ°á»ng]

---

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ váº¥n Ä‘á», vui lÃ²ng táº¡o issue trong repository hoáº·c liÃªn há»‡ nhÃ³m.
