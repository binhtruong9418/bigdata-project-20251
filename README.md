# Zillow Real Estate Data Pipeline - NhÃ³m 14

[BÃ¡o cÃ¡o BTL BigData - NhÃ³m 14 - Lá»›p 154050](https://docs.google.com/document/d/1Svi3nbpFZvkNQm9AJzbJgJ29YFHlan6-cYaCBs4nb8U/edit?tab=t.0)

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n real-time sá»­ dá»¥ng cÃ´ng nghá»‡ Big Data. Pipeline bao gá»“m:

1. **Data Ingestion**: Táº¡o dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t California vÃ  gá»­i vÃ o Kafka
2. **Stream Processing**: Xá»­ lÃ½ real-time vá»›i Spark Structured Streaming
3. **Batch Processing**: LÆ°u trá»¯ batch data vÃ o HDFS vÃ  Cassandra
4. **Machine Learning**: Train model dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i Random Forest
5. **Analytics**: Aggregation theo thÃ nh phá»‘, thá»i gian

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
Producer (Python)
    â†“
Kafka Cluster (3 brokers)
    â†“
    â”œâ”€â”€ Spark Structured Streaming â†’ Console Output (Real-time Analytics)
    â”‚
    â””â”€â”€ Batch Consumer â†’ HDFS
            â†“
        Batch Processing (PySpark)
            â†“
        Cassandra â†’ ML Training (sparkML.py)
```

### CÃ¡c thÃ nh pháº§n:

- **Kafka Producer**: Sinh dá»¯ liá»‡u giáº£ láº­p nhÃ  Ä‘áº¥t (price, bedrooms, city, ...)
- **Kafka Cluster**: 3 brokers (ports 9092, 9093, 9094) + ZooKeeper
- **Spark Streaming**: Xá»­ lÃ½ real-time, tÃ­nh toÃ¡n metrics theo time window
- **Batch Consumer**: Äá»c tá»« Kafka, lÆ°u batch vÃ o HDFS
- **HDFS**: LÆ°u trá»¯ phÃ¢n tÃ¡n dá»¯ liá»‡u batch
- **Batch Processing**: Load data tá»« HDFS vÃ o Cassandra (tá»± Ä‘á»™ng setup database)
- **Cassandra**: NoSQL database cho ML pipeline
- **ML Training**: Huáº¥n luyá»‡n model dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (Random Forest)

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m cáº§n thiáº¿t:
- **Python 3.11** (KHUYáº¾N NGHá»Š - Full compatibility)
- **Java** >= 8 (JDK)
- **Docker** vÃ  **Docker Compose**
- **Git** (Ä‘á»ƒ clone project)

**Táº¡i sao Python 3.11?**
- âœ… Há»— trá»£ Ä‘áº§y Ä‘á»§ táº¥t cáº£ thÆ° viá»‡n (Kafka, PySpark, Cassandra)
- âœ… cassandra-driver hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… PySpark 3.5.0 stable vá»›i Cassandra connector
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i toÃ n bá»™ Big Data stack

**LÆ°u Ã½**:
- Python 3.13 chÆ°a Ä‘Æ°á»£c cassandra-driver há»— trá»£
- PySpark 4.0+ chÆ°a tÆ°Æ¡ng thÃ­ch vá»›i Cassandra Spark Connector
- Sá»­ dá»¥ng Python 3.11 + PySpark 3.5.0 Ä‘á»ƒ trÃ¡nh váº¥n Ä‘á» tÆ°Æ¡ng thÃ­ch

### Há»‡ Ä‘iá»u hÃ nh:
- Windows 10/11, Linux, hoáº·c MacOS

---

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone project

```bash
git clone <repository-url>
cd bigdata-project-20251
```

### BÆ°á»›c 2: Táº¡o mÃ´i trÆ°á»ng áº£o vÃ  cÃ i Ä‘áº·t dependencies

#### Windows:
```bash
python -m venv .venv
.venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

#### Linux/Mac:
```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

**LÆ°u Ã½ quan trá»ng**: Náº¿u gáº·p lá»—i import khi cháº¡y PySpark, xÃ³a `.venv` vÃ  táº¡o láº¡i tá»« Ä‘áº§u:
```bash
# Windows
rmdir /s /q .venv
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Linux/Mac
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### BÆ°á»›c 3: Setup Hadoop cho Windows (chá»‰ Windows)

ÄÃ£ Ä‘Æ°á»£c tá»± Ä‘á»™ng setup trong code. Hadoop binaries sáº½ Ä‘Æ°á»£c táº£i tá»± Ä‘á»™ng vÃ o folder `hadoop/bin/`.

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Docker services

```bash
docker-compose up -d
```

Chá» khoáº£ng 30-60 giÃ¢y Ä‘á»ƒ cÃ¡c services khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

### BÆ°á»›c 5: Kiá»ƒm tra cÃ¡c services

```bash
docker-compose ps
```

Äáº£m báº£o táº¥t cáº£ containers Ä‘ang cháº¡y (status: Up).

---

## ğŸ¯ Cháº¡y dá»± Ã¡n

### Workflow hoÃ n chá»‰nh:

```
1. Start Docker services
2. Run Producer (táº¡o data)
3. Run Batch Consumer (lÆ°u vÃ o HDFS)
4. Run Streaming Consumer (real-time analytics)
5. Wait 10-15 minutes (collect data)
6. Run Batch Processing (HDFS â†’ Cassandra, auto setup database)
7. Run ML Training (train model tá»« Cassandra)
```

### CÃ¡ch 1: Cháº¡y tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

#### Windows:
```bash
.\start.bat
```

#### Linux/Mac:
```bash
chmod +x start.sh
./start.sh
```

Script sáº½ tá»± Ä‘á»™ng cháº¡y:
- Kafka Producer
- Batch Consumer (HDFS)
- Spark Streaming Consumer

### CÃ¡ch 2: Cháº¡y tá»«ng thÃ nh pháº§n riÃªng láº»

#### Terminal 1 - Kafka Producer:
```bash
python kafka/producer.py
```

#### Terminal 2 - Batch Consumer (HDFS):
```bash
python kafka/consumer_batch.py
```

#### Terminal 3 - Streaming Consumer (Spark):
```bash
python kafka/consumer_structured_stream.py
```

### Machine Learning Workflow:

#### BÆ°á»›c 1: Collect data (Ä‘á»£i 10-15 phÃºt)
```bash
# Producer, Batch Consumer, vÃ  Spark Streaming Ä‘ang cháº¡y
# Äá»£i Ä‘á»ƒ data Ä‘Æ°á»£c collect vÃ o HDFS
# Kiá»ƒm tra: http://localhost:9870 â†’ Utilities â†’ Browse the file system â†’ /data/kafka_messages
```

#### BÆ°á»›c 2: Load data tá»« HDFS vÃ o Cassandra
```bash
python spark/batch_processing.py
```

**TÃ­nh nÄƒng má»›i**:
- âœ… Tá»± Ä‘á»™ng táº¡o Cassandra keyspace `finaldata1`
- âœ… Tá»± Ä‘á»™ng táº¡o table `data2` vá»›i schema phÃ¹ há»£p
- âœ… KhÃ´ng cáº§n cháº¡y setup riÃªng
- âœ… Hiá»ƒn thá»‹ progress bar khi xá»­ lÃ½ nhiá»u files
- âœ… Error handling tá»‘t hÆ¡n, retry logic
- âœ… Summary report chi tiáº¿t (success/failed counts)

Output máº«u:
```
============================================================
Zillow Batch Processing - HDFS to Cassandra
============================================================

[Setup] Configuring Cassandra database...
[OK] Cassandra keyspace 'finaldata1' and table 'data2' ready

[1/4] Connecting to HDFS...
[OK] Connected to HDFS

[2/4] Searching for data files...
[OK] Found 2984 files to process

[3/4] Initializing Spark session...
[OK] Spark session created

[4/4] Processing and loading data to Cassandra...
[Progress] Processed 100/2984 files...
[Progress] Processed 200/2984 files...

============================================================
BATCH PROCESSING COMPLETED
============================================================

Total files found: 2984
Successfully processed: 2980
Failed: 4

Data written to: Cassandra keyspace 'finaldata1', table 'data2'
```

#### BÆ°á»›c 3: Kiá»ƒm tra data trong Cassandra (Optional)
```bash
python check_cassandra_data.py
```

Script sáº½:
- Káº¿t ná»‘i tá»›i Cassandra
- Äáº¿m sá»‘ lÆ°á»£ng records trong table `finaldata1.data2`
- Hiá»ƒn thá»‹ sample data
- ÄÆ°a ra khuyáº¿n nghá»‹ cÃ³ nÃªn train ML model hay chÆ°a

#### BÆ°á»›c 4: Train ML model
```bash
python spark/sparkML.py
```

Model sá»­ dá»¥ng Random Forest Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn:
- Sá»‘ phÃ²ng ngá»§ (bedrooms)
- Sá»‘ phÃ²ng táº¯m (bathrooms)
- Diá»‡n tÃ­ch (livingarea)
- Loáº¡i nhÃ  (hometype)
- ThÃ nh phá»‘ (city)

### Dá»«ng há»‡ thá»‘ng:

#### Windows:
```bash
.\stop.bat
```

#### Linux/Mac:
```bash
chmod +x stop.sh
./stop.sh
```

Hoáº·c thá»§ cÃ´ng:
```bash
# Nháº¥n Ctrl+C á»Ÿ má»—i terminal Ä‘á»ƒ dá»«ng cÃ¡c consumer/producer

# Dá»«ng Docker services
docker-compose down
```

---

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Producer Output:
```
Sent data: {'timestamp': 1763174687104, 'zpid': 336424216, 'city': 'Los Angeles', 'price': 7578356, ...}
Sent data: {'timestamp': 1763174687952, 'zpid': 261109533, 'city': 'Glendale', 'price': 7632629, ...}
```

### Batch Consumer Output:
```
2025-11-15 10:06:05,815 - __main__ - INFO - Batch of 10 messages saved to /data/kafka_messages/2025/11/15/10_06_05_843515_batch.json
```

### Spark Streaming Output:
```
+----------------------------------------------+--------------+-------------+-----------------+
|window                                        |city          |average_price|total_bedrooms   |
+----------------------------------------------+--------------+-------------+-----------------+
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Los Angeles   |6022806.0    |6                |
|{2025-11-15 10:00:00, 2025-11-15 10:01:00}   |Beverly Hills |8109473.0    |11               |
+----------------------------------------------+--------------+-------------+-----------------+
```

### Batch Processing Output:
```
Sample data (first file):
+---------+--------------+-------------+--------+------------+---------+--------+----------+----------+-----------------+--------------------+-------------------------+
|zpid     |city          |hometype     |price   |lotareavalue|bathrooms|bedrooms|livingarea|isfeatured|isshowcaselisting|newconstructiontype |listingsubtype_is_newhome|
+---------+--------------+-------------+--------+------------+---------+--------+----------+----------+-----------------+--------------------+-------------------------+
|169024753|Santa Monica  |MULTI_FAMILY |795129  |5.4735      |2        |11      |7142      |true      |true             |BUILDER_SPEC        |true                     |
+---------+--------------+-------------+--------+------------+---------+--------+----------+----------+-----------------+--------------------+-------------------------+

[Progress] Processed 100/2984 files...
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
bigdata-project-20251/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py                  # Kafka producer
â”‚   â”œâ”€â”€ consumer_batch.py            # Batch consumer â†’ HDFS
â”‚   â””â”€â”€ consumer_structured_stream.py # Spark streaming consumer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ batch_processing.py          # HDFS â†’ Cassandra (optimized, auto setup)
â”‚   â”œâ”€â”€ sparkML.py                   # ML model training
â”‚   â””â”€â”€ sparkML_note.txt             # Notes
â”œâ”€â”€ hadoop/
â”‚   â””â”€â”€ bin/                         # Hadoop binaries (Windows only)
â”œâ”€â”€ check_cassandra_data.py          # Verify Cassandra data
â”œâ”€â”€ docker-compose.yml               # Docker services configuration
â”œâ”€â”€ requirements.txt                 # Python dependencies (PySpark 3.5.0)
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ start.bat                        # Windows startup script
â”œâ”€â”€ start.sh                         # Linux/Mac startup script
â”œâ”€â”€ stop.bat                         # Windows stop script
â”œâ”€â”€ stop.sh                          # Linux/Mac stop script
â””â”€â”€ README.md                        # This file
```

---

## ğŸŒ Web UIs

Sau khi khá»Ÿi Ä‘á»™ng Docker services, cÃ³ thá»ƒ truy cáº­p:

- **Spark Master**: http://localhost:8080
- **Spark Worker**: http://localhost:8081
- **HDFS NameNode**: http://localhost:9870
  - Browse files: Utilities â†’ Browse the file system â†’ /data/kafka_messages

---

## ğŸ“¦ Dependencies chÃ­nh

```
# Core Big Data Processing
pyspark==3.5.0                      # TÆ°Æ¡ng thÃ­ch vá»›i Cassandra connector
py4j==0.10.9.7

# Kafka
kafka-python-ng                      # Kafka client (Python 3.11+ compatible)

# Data Processing
pandas
numpy

# Storage Connectors
hdfs                                 # HDFS client
cassandra-driver                     # Cassandra client (Python 3.11 compatible)
```

**Version Compatibility Matrix**:
| Component | Version | Reason |
|-----------|---------|--------|
| PySpark | 3.5.0 | Compatible with Cassandra connector 3.5.0 |
| Cassandra Connector | 3.5.0 (Scala 2.12) | Matches Spark 3.5.0 Scala version |
| Python | 3.11 | Full cassandra-driver support |

---

## ğŸ› Troubleshooting

### 1. Import Error: `cannot import name 'is_remote_only'`
**NguyÃªn nhÃ¢n**: Virtual environment bá»‹ corrupt vá»›i mixed PySpark versions

**Giáº£i phÃ¡p**:
```bash
# Windows
deactivate
rmdir /s /q .venv
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Linux/Mac
deactivate
rm -rf .venv
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Cassandra Connection Error
**NguyÃªn nhÃ¢n**: Cassandra chÆ°a sáºµn sÃ ng

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Cassandra status
docker-compose ps

# Restart Cassandra
docker-compose restart cassandra

# Äá»£i 30-60 giÃ¢y rá»“i thá»­ láº¡i
```

### 3. Spark Cassandra Connector Error: `NoClassDefFoundError: scala/$less$colon$less`
**NguyÃªn nhÃ¢n**: Sai Scala version trong connector

**Giáº£i phÃ¡p**: ÄÃ£ fix trong code, sá»­ dá»¥ng `_2.12` thay vÃ¬ `_2.13`

### 4. Docker containers khÃ´ng start
```bash
docker-compose down
docker-compose up -d
```

### 5. Port Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng
```bash
# Windows
netstat -ano | findstr :9092
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :9092
kill -9 <PID>
```

### 6. HDFS khÃ´ng accessible
```bash
# Kiá»ƒm tra HDFS NameNode
docker logs namenode

# Restart HDFS
docker-compose restart namenode datanode
```

### 7. Kafka connection timeout
Äá»£i thÃªm 30-60 giÃ¢y Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

---

## ğŸ”§ Optimizations

### Batch Processing Script (`batch_processing.py`):
1. **Integrated Cassandra Setup**: Tá»± Ä‘á»™ng táº¡o keyspace vÃ  table, khÃ´ng cáº§n script riÃªng
2. **Modular Design**: Functions cho tá»«ng task (HDFS, Spark, Transform)
3. **Better Error Handling**: Try-catch cho tá»«ng file, khÃ´ng dá»«ng náº¿u 1 file lá»—i
4. **Progress Tracking**: Hiá»ƒn thá»‹ progress má»—i 100 files
5. **Summary Report**: Tá»•ng káº¿t success/failed counts

### Version Compatibility:
- Downgrade tá»« PySpark 4.0.1 â†’ 3.5.0 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i Cassandra connector
- Sá»­ dá»¥ng Scala 2.12 connector thay vÃ¬ 2.13
- Python 3.11 cho full cassandra-driver support

---

## ğŸ‘¥ NhÃ³m thá»±c hiá»‡n

**NhÃ³m 14 - Lá»›p 154050**

---

## ğŸ“ License

Dá»± Ã¡n há»c táº­p - Äáº¡i há»c [TÃªn trÆ°á»ng]

---

## ğŸ“ LiÃªn há»‡

Náº¿u cÃ³ váº¥n Ä‘á», vui lÃ²ng táº¡o issue trong repository hoáº·c liÃªn há»‡ nhÃ³m.

---

## ğŸ“ Learning Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Cassandra Documentation](https://cassandra.apache.org/doc/)
- [HDFS Architecture](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)
- [Spark-Cassandra Connector](https://github.com/apache/cassandra-spark-connector)
